{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Deadfish-hk/AcuCount_imageset/blob/main/telegram_bot_colab_mmrotate_github.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The starting script of the telegram detection bot.\n",
        "# You can replace the .pth file and the.config into any object of your interest, after custom training."
      ],
      "metadata": {
        "id": "NQGtkDzdryK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHTuvjUUx__4"
      },
      "source": [
        "# Mount the google drive if you are working with cloud drive. Comment out this section if you are working with local file system.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the AcuCount Repo for dataset and configs\n",
        "!git clone https://github.com/Deadfish-hk/AcuCount_imageset.git"
      ],
      "metadata": {
        "id": "ZKkWDoLOt393"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the AcuCount model stored in Google drive for demostration\n",
        "!pip install gdown\n",
        "!gdown https://drive.google.com/uc?id=11qKHCZYwAelya_bHDej4xDTKu7k9eb8z"
      ],
      "metadata": {
        "id": "mHj6Qekut4OA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8wHnCuQqEgC"
      },
      "source": [
        "# install python-telegram-bot (13.14)\n",
        "!pip install 'python-telegram-bot==13.14'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies: (use cu116 because colab has CUDA 11.6)\n",
        "!pip install torch==1.12.1+cu116 torchvision==0.13.1+cu116 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# Install mmcv-full and mmdetection thus we could use CUDA operators\n",
        "!pip3 install -U openmim\n",
        "!mim install mmcv-full==1.6.2\n",
        "!mim install mmdet==2.26.0\n",
        "\n",
        "# Install mmrotate\n",
        "!git clone --branch v0.3.4 https://github.com/open-mmlab/mmrotate.git\n",
        "%cd mmrotate\n",
        "!pip install -e ."
      ],
      "metadata": {
        "id": "zm23ba3WvMOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Pytorch installation\n",
        "import torch, torchvision\n",
        "print(torch.__version__, torch.cuda.is_available())\n",
        "\n",
        "# Check MMRotate installation\n",
        "import mmrotate\n",
        "print(mmrotate.__version__)\n",
        "\n",
        "# Check MMDetection installation\n",
        "import mmdet\n",
        "print(mmdet.__version__)\n",
        "\n",
        "# Check mmcv installation\n",
        "from mmcv.ops import get_compiling_cuda_version, get_compiler_version\n",
        "print(get_compiling_cuda_version())\n",
        "print(get_compiler_version())"
      ],
      "metadata": {
        "id": "9-nyi-WvvTlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mmcv\n",
        "from mmcv.runner import load_checkpoint\n",
        "from mmdet.apis import inference_detector, show_result_pyplot\n",
        "from mmrotate.models import build_detector\n",
        "\n",
        "# Choose to use a config and initialize the detector\n",
        "config = '/content/AcuCount_imageset/AcuCount_detection_model_config.py'   # Insert link to Config file here\n",
        "# Setup a checkpoint file to load\n",
        "checkpoint = '/content/AcuCount_detection_model.pth' # Insert link to checkpoint file here\n",
        "\n",
        "# Set the device to be used for evaluation\n",
        "device='cuda:0'\n",
        "\n",
        "# Load the config\n",
        "config = mmcv.Config.fromfile(config)\n",
        "# Set pretrained to be None since we do not need pretrained model here\n",
        "config.model.pretrained = None\n",
        "\n",
        "# Initialize the detector\n",
        "model = build_detector(config.model)\n",
        "\n",
        "# Load checkpoint\n",
        "checkpoint = load_checkpoint(model, checkpoint, map_location=device)\n",
        "\n",
        "# Set the classes of models for inference\n",
        "model.CLASSES = checkpoint['meta']['CLASSES']\n",
        "\n",
        "# We need to set the model's cfg for inference\n",
        "model.cfg = config\n",
        "\n",
        "# Convert the model to GPU\n",
        "model.to(device)\n",
        "# Convert the model into evaluation mode\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "MVkImFnFvb5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def counting(result):     # Count the number of detections found.\n",
        "  count = 0\n",
        "  count_order = 0\n",
        "  count_list = result[0]\n",
        "  for item in np.nditer(count_list):\n",
        "    box_score = count_list[count_order][5]\n",
        "    if box_score > 0.5:\n",
        "      count = count + 1\n",
        "      count_order = count_order + 1\n",
        "      if count_order == len(count_list):\n",
        "        break\n",
        "    if box_score < 0.5:\n",
        "      count_order = count_order + 1\n",
        "      if count_order == len(count_list):\n",
        "        break\n",
        "  return count"
      ],
      "metadata": {
        "id": "neImXcKw4NSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFYIBgo5qAQP"
      },
      "source": [
        "import telegram\n",
        "import numpy as np\n",
        "from telegram.ext import Updater, CommandHandler, MessageHandler, Filters\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from PIL import Image\n",
        "import time\n",
        "import mmcv\n",
        "from mmcv.runner import load_checkpoint\n",
        "from mmdet.apis import inference_detector, show_result_pyplot\n",
        "from mmrotate.models import build_detector\n",
        "import os\n",
        "\n",
        "bot = telegram.Bot(token='xx')  # Insert your telegram bot token here\n",
        "\n",
        "updater = Updater(token='xx', use_context=True)  # Insert your telegram bot token here\n",
        "\n",
        "dispatcher = updater.dispatcher\n",
        "\n",
        "# Set up logging\n",
        "logger = logging.getLogger()\n",
        "fhandler = logging.FileHandler(filename='/content/drive/MyDrive/processed_images_record/timeevent.log', mode='a')  # Setup your log file for recording end-end counting time\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "fhandler.setFormatter(formatter)\n",
        "logger.addHandler(fhandler)\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "dim = (960, 1280)\n",
        "\n",
        "# Welcome message\n",
        "def start(update, context):\n",
        "    context.bot.send_message(chat_id=update.effective_chat.id, text=\"Send me a photo with needles then I will do the detection for you!\"\n",
        "                            )\n",
        "    context.bot.send_photo(chat_id=update.effective_chat.id, photo=open('/content/drive/MyDrive/telegrambot_config/start_image.png', 'rb')) # Place your example detection here\n",
        "\n",
        "def detection_image(update, context):\n",
        "    photos = update.message.photo\n",
        "    documents = update.message.document\n",
        "    # timestamp in UTC time\n",
        "    now = datetime.now()\n",
        "    timestamp_date = now.strftime(\"%Y%m%d\")\n",
        "    timestamp_time = now.strftime(\"%H%M%S\")\n",
        "    if photos:\n",
        "        user = update.message.from_user\n",
        "        update.message.reply_text(\"I have received your image. Now detecting needles..\\nIt may take 5 seconds to process.. \")\n",
        "        logger.info(f\"Photo received from {user.first_name} {user.last_name}\")\n",
        "        start_time = time.time()\n",
        "        # Download the 960x1280 photo\n",
        "        photo_id = photos[-1].file_id\n",
        "        context.bot.get_file(photo_id).download(f\"/content/drive/MyDrive/downloaded_images_record/{timestamp_date}-{timestamp_time}.jpg\")  # Download the incoming image in your Gdrive directory. By default it is directed to be /downloaded_images_record under your Gdrive.\n",
        "        image_path = f'/content/drive/MyDrive/downloaded_images_record/{timestamp_date}-{timestamp_time}.jpg' # Download the incoming image in your Gdrive directory. By default it is directed to be /downloaded_images_record under your Gdrive.\n",
        "        img = mmcv.imread(image_path)\n",
        "        result = inference_detector(model, img)\n",
        "        count = counting(result)\n",
        "        show_result_pyplot(model, img, result, score_thr=0.5, out_file= f'/content/drive/MyDrive/processed_images_record/{count}-{timestamp_date}-{timestamp_time}.jpg') # Process the incoming image and store it in your Gdrive. By default it is directed to be /processed_images_record under your Gdrive.\n",
        "    elif documents.mime_type == 'image/heic' or 'image/jpeg' or 'image/png':\n",
        "        user = update.message.from_user\n",
        "        update.message.reply_text(\"I have received your image. Now detecting needles..\\nIt may take 5 seconds to process.. \")\n",
        "        logger.info(f\"H-res Photo received from {user.first_name} {user.last_name}\")\n",
        "        start_time = time.time()\n",
        "        # Download the uncompressed photo\n",
        "        context.bot.get_file(documents.file_id).download(f\"/content/drive/MyDrive/downloaded_images_record/{timestamp_date}-{timestamp_time}.jpg\") # Download the incoming image in your Gdrive directory. By default it is directed to be /downloaded_images_record under your Gdrive.\n",
        "        simg = Image.open(f\"/content/drive/MyDrive/downloaded_images_record/{timestamp_date}-{timestamp_time}.jpg\") # Download the incoming image in your Gdrive directory. By default it is directed to be /downloaded_images_record under your Gdrive.\n",
        "        #simg_resized = simg.resize (dim)\n",
        "        #simg_resized.save(f\"/content/drive/MyDrive/downloaded_images_record/{timestamp_date}-{timestamp_time}_resized.jpg\") # Reserved for any necessary resizing effort\n",
        "        image_path = f'/content/drive/MyDrive/downloaded_images_record/{timestamp_date}-{timestamp_time}.jpg'\n",
        "        img = mmcv.imread(image_path)\n",
        "        result = inference_detector(model, img)\n",
        "        count = counting(result)\n",
        "        show_result_pyplot(model, img, result, score_thr=0.5, out_file= f'/content/drive/MyDrive/processed_images_record/{count}-{timestamp_date}-{timestamp_time}.jpg') # Process the incoming image and store it in your Gdrive. By default it is directed to be /processed_images_record under your Gdrive.\n",
        "\n",
        "    update.message.reply_text(f\"Needle count:{count}\")\n",
        "    update.message.reply_photo(open(f'/content/drive/MyDrive/processed_images_record/{count}-{timestamp_date}-{timestamp_time}.jpg','rb'))\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    logger.info(f\"Photo sent to {user.first_name} {user.last_name}, elapsed time:{elapsed_time}\")\n",
        "    # Rename image for easy reference\n",
        "    OLD_DOWNLOAD = f'/content/drive/MyDrive/downloaded_images_record/{timestamp_date}-{timestamp_time}.jpg'\n",
        "    NEW_DOWNLOAD = f'/content/drive/MyDrive/downloaded_images_record/{timestamp_date}-{timestamp_time}_{user.first_name}_{user.last_name}.jpg'\n",
        "    OLD_PROCESS = f'/content/drive/MyDrive/processed_images_record/{count}-{timestamp_date}-{timestamp_time}.jpg'\n",
        "    NEW_PROCESS = f'/content/drive/MyDrive/processed_images_record/{count}-{timestamp_date}-{timestamp_time}_{user.first_name}_{user.last_name}.jpg'\n",
        "    os.renames (OLD_DOWNLOAD,NEW_DOWNLOAD)\n",
        "    os.renames (OLD_PROCESS,NEW_PROCESS)\n",
        "\n",
        "start_handler = CommandHandler('start', start)\n",
        "dispatcher.add_handler(start_handler)\n",
        "dispatcher.add_handler(MessageHandler(Filters.photo | Filters.document, detection_image))\n",
        "\n",
        "updater.start_polling()\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CatBblyXoT15"
      },
      "source": [
        "# Stop the running telegram bot\n",
        "updater.stop()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}